Even if we can’t “see them”, programs like ChatGPT's ability to emote like a human gives some people an eerie feeling. Scientists have a few theories on why this phenomenon happens.
What does it mean to be human? When a robot asks that question, it might elicit feelings of unease—indeed, when artificial beings start acting or looking a little too human, you might experience the uncanny valley phenomenon.
Gareth Edwards’ new feature The Creator poses this fundamental question. In the film, our worst fears about artificial intelligence come true. Though the lightning fast development of AI may seem recent, AI fears may touch on basic human instincts to survive.
Although the concept of the uncanny valley has existed for half a century, scientists still debate why fabricated people cause us so much discomfort. Theories range from our instinct to avoid disease, to perceiving a threat to our sense of humanity. 
Meanwhile, roboticists and AI researchers are working hard to cross the uncanny valley, hoping to bring social robots into everyday life. In the future, robots and AI could be waiting tables, caring for the elderly, teaching kids to read, or sitting in as patients in medical school. Whether and how robots manage to cross the valley could greatly impact how we interact with them in the future.
The concept of the uncanny valley was first coined by roboticist Masahiro Mori in 1970. In an essay, Mori proposed that robots become more likable as they gain humanlike qualities (think WALL-E). But when they get too humanlike (i.e. the valley), they start getting creepy. Then, when they get nearly indistinguishable from humans, they get likable again.
Trailer: The Creator
Mori’s theories were based on his personal experience, but have been highly influential, says roboticist Karl MacDorman, an associate dean at the Indiana University School of Informatics and Computing, and translator of Mori’s 1970 essay. Still, scientific support for Mori’s uncanny valley has been mixed, and it should be thought of as a heuristic rather than a hard and fast rule, says MacDorman. 
Over the years, researchers have been finding uncanny valleys everywhere. There’s an uncanny valley for human and synthetic voices, one for robotic animals, and even one for houses. 
In a recent study, MacDorman and cognitive psychologist Alex Diel found the most support for a theory called configural processing, the idea that uncanny valley reactions are caused by our sensitivity to the positioning and size of human facial features. Perceptual mismatch, another related theory, says that we become uncomfortable when we pick up on mismatched features, like realistic eyes but unrealistic skin. This particular incongruity is a common problem for stable diffusion-generated AI images. 
From an evolutionary standpoint, these sensitivities may trigger an instinct to avoid a potential threat. Diel explains we may see imperfections in a human replica as a sign they might be physically ill or a potential source of contagious illness—and that triggers our disgust response. Mate selection theory is similar: it posits we're averse to humanlike robots because our instinct tells us their imperfections show they wouldn't be good mates. Another theory is that artificial agents unsettle us because they seem to have come alive unnaturally, like zombies, and make us think about our own death.
Some cognitive explanations of the uncanny valley include the idea that we assign humanlike qualities or a mind to artificial people, and this can cause cognitive dissonance and confusion since we don’t know if we should treat them as human, or trust them to behave as such. 
More recently, evidence has suggested that artificial beings are unsettling because they challenge our beliefs about the uniqueness of human ability, like reasoning, logic, and emotions. In a recent study, participants reported that interactions with humanlike androids made them question what it means to be human. Dawid Ratajzyc, a professor at Adam Mickiewicz University, who conducted the study, says maybe “robots can tell us more about ourselves than about robots.”
An infamous 1988 video shows a CGI baby playing with toys. MacDorman says it’s a prime example of the uncanny valley, explaining that the feeling a viewer gets “is very visceral, automatic, and uncontrolled.” He distinguishes this reaction from one you might get from talking with a chatbot, which involves thinking and deliberation, MacDorman says. “I don't think it's really a theory of the uncanny valley as defined by Mitsuhiro Mori.”
Ratajzyc however, sees them as the same. In fact, he thinks that any artificial agent, from a robot to a chatbot, can elicit uncanny valley reactions. He points to a recent study that showed that simple text chatbots seem less creepy than those with a virtual humanlike avatar “speaking” to a user—and the more humanlike the avatar seems, the more repulsive the chatbot gets.
There is some evidence from brain imaging studies that these two types of interactions– automatic, sensory responses versus interactions that require thinking and deliberation—use different parts of the brain, and that we might use more analytical parts of the brain in social interactions with robots than with humans. 
Nadine the social robot can greet you and remember the conversations you’ve had previously. She was unveiled to the world almost seven years ago, and has been working at an insurance company in Singapore. Since February of this year, 100 million people have used ChatGPT. As we interact more with androids and AI, and they improve in realism, will they get any less uncanny?
It’s hard to say, says Bilge Mutlu, a professor of computer science at the University of Wisconsin Madison. While the researchers expect that with repeated exposure, the uncanny valley reaction might diminish, Mutlu says that for him, the feeling has only gotten stronger. 
MacDorman also thinks there might be something generational going on. He recalls that in 2020 when he was unveiling Geminoid H1, the android that roboticist Hiroshi Ishiguro made of himself, an older man came into the room and asked where the android was—while he was standing right next to it.
Mori had a simple solution for staying out of the uncanny valley: don’t build humanlike robots. But many modern-day roboticists, like MacDorman, haven’t been satisfied with that solution. They’re trying to make robots look and act more human, both to ask fundamental questions about humans, and so that robots can seamlessly integrate into human life. 
But that comes with ethical questions: how human should a nonhuman robot be? Should people know that they’re interacting with an artificial agent, and how much information should an artificial intelligence have about us?
Mutlu thinks it's not necessary for all robots to look and behave exactly like humans. We should think carefully about what purposes we use robotic agents for, and design appropriately, he says.
We also don’t need them to make important decisions that we can make ourselves, he adds. Even now, artificial intelligence is being used to decide insurance claims and whether to put people in jail. He hopes roboticists and AI researchers focus more on building helpers that either restore or go beyond human ability.
Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2023 National Geographic Partners, LLC. All rights reserved
