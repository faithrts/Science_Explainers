LONDON, May 19 (Reuters Breakingviews) - “My colleagues, they study artificial intelligence,” the Israeli psychologist Amos Tversky once quipped. “Me, I study natural stupidity.” The co-founder of behavioural economics, who died in 1996, did not live to see 2023, when more of his academic colleagues jumped on the AI bandwagon along with venture capitalists, corporate leaders, and stock jocks. But investors should pay closer attention to Tversky’s specialisation. Behavioural economics, which studies how psychological, emotional, and social factors affect human decision-making, has some important pointers for those hoping to cash in on AI.
The first lesson is the most obvious: beware of bubbles. Since OpenAI released its ChatGPT chatbot last November, the steady flow of capital into all things AI-related has turned into a torrent. Shares in Nvidia (NVDA.O), the world’s leading maker of chips used in creating AI, have surged more than 100% over the last six months. Software giant Microsoft (MSFT.O) has gained almost $500 billion in market value since announcing in February that it was incorporating AI into its Bing search engine. Investors in Alphabet (GOOGL.O) added a cool $60 billion to the Google owner’s worth in a single day last week after CEO Sundar Pichai unveiled its new AI offering at the company’s annual I/O conference.
Indeed, enthusiasm about AI has become the one ray of light piercing the stock market gloom created by the record-breaking rise in U.S. interest rates. SocGen analyst Manish Kabra calculated last week that, excluding AI-related gains, the S&P 500 Index (.SPX) would be down 2% year-to-date. Instead, it was up 8%. The boom even has macroeconomic consequences. Irish Finance Minister Michael McGrath last week unveiled plans for a new 90-billion-euro sovereign wealth fund, largely funded by a corporate tax windfall from tech giants such as Apple (AAPL.O) and Microsoft which are domiciled in the country.
For other companies, perceived vulnerabilities to AI can spell doom. Shares in Chegg (CHGG.N) cratered earlier this month when the maker of study materials admitted that so-called large language models such as ChatGPT were eating into its market.
Orthodox asset pricing models suggest these wild gyrations reflect changing but rational assessments of future profitability. But behavioural economics has long furnished an alternative explanation by enumerating a rogues’ gallery of systematic flaws in human decision-making. These range from herding and overconfidence to confirmation bias and the fear of missing out. It’s a good moment for investors to be especially alert to the tendency of natural stupidity to drive stock market valuations to unrealistic – and therefore ultimately unprofitable – extremes.
However, the most important lessons of behavioural economics relate to a more fundamental question: Will the new generation of AI do what it promises? The technology has already achieved some seriously impressive results. In November 2020 Google DeepMind’s AlphaFold stunned the scientific world by achieving a step-change in one of the grand challenges in molecular biology. It predicted the structures into which proteins “fold” based only on the sequences of their constituent amino acids. Venki Ramakrishnan, the Nobel Laureate and then President of Britain’s Royal Society, called it an advance which will “fundamentally change biological research”.
AlphaFold demonstrated what is widely understood to be AI’s greatest strength: its ability to recognise patterns which elude both human intuition and traditional statistical analysis, and then to leverage these patterns for predictive purposes. The same capability characterised AI’s stunning achievements in defeating human opponents in strategic games such as chess and Go, and has enabled ChatGPT to produce eerily coherent prose.
The big unknown is whether AI will be able to replicate this extraordinary predictive ability in areas of commercial, financial, and political life where the rules are fuzzier. Behavioural economics offers some cautionary tales for such attempts to apply AI in the wild.
One potential gremlin is the problem of so-called sampling bias when building predictive models based on statistical learning. The issue is that datasets used to train models may omit rare but consequential events. For example, stock market returns can be affected by a small number of rare but extreme movements in share prices. As a result, quantitative trading firms have often eschewed pure data-mining strategies in favour of approaches in which the probability of so-called tail risks is assumed rather than learned. Less technically-minded investors adopt their own version of the same tactic when they deploy simple heuristics such as the legendary investor Benjamin Graham’s “margin of safety”.
Behavioural economists described the problem of sampling bias while studying how humans learn. But neural networks may suffer similar shortcomings. Intelligent machines, no less than naturally stupid humans, will have to confront the irritating fact that the absence of evidence is almost never evidence of absence.
Then there is perhaps the most frustrating of all problems when it comes to modelling and manipulating human behaviour: Goodhart’s Law. This is the paradox, first articulated by Bank of England official Charles Goodhart in 1975, that when a metric becomes a policy target it ceases to be a reliable metric. For example, monetary aggregates were once good predictors of inflation. But when central banks adopted targets based on those figures the stable correlation disappeared.
The root of this problem is that human systems are intrinsically adaptive in a way that physical systems are not. People understand and attempt to outsmart efforts to predict their behaviour if they sense it is against their interests. The amino acids involved in protein-folding do not.
Once again, these practical challenges are well documented in the field of investing. Securities trading is a zero-sum game: one investor’s capital gain is another’s capital loss. As a result, there is a powerful and automatic incentive for the rest of the market to adapt and nullify historically successful trading rules as soon as they are identified. Goodhart’s Law explains why the excess returns enjoyed by systematic investment strategies typically erode over time. Whether AI will fare any better at escaping its gravity remains an open question.
Investors would be mad to ignore the spectacular results that AI has produced so far. But when it comes to broader applications, they should tread carefully. Artificial intelligence may have more in common with natural stupidity than humans - or machines – currently think.
Follow @felixmwmartin on Twitter
(The author is a Reuters Breakingviews columnist. The opinions expressed are their own.)
Editing by Peter Thal Larsen and Pranav Kiran
Our Standards: The Thomson Reuters Trust Principles.
