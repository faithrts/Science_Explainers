{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithrts/Science_Explainers/blob/main/analysis/word_and_pos_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Il4s_vHhFaj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKJMJPjMzJj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d74c39-51a9-4a01-8154-78641becb11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# to download files\n",
        "from google.colab import files\n",
        "\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "import codecs\n",
        "import pickle\n",
        "\n",
        "# for word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# scipy\n",
        "from scipy.stats import chi2  ## for stats\n",
        "from scipy import spatial     ## for cosine similarity\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# silencing the warnings\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing datasets"
      ],
      "metadata": {
        "id": "1jFpPaq5PE03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### cloning git repos\n",
        "\n",
        "!git clone https://github.com/faithrts/Science_Explainers\n",
        "#!git clone https://github.com/faithrts/Short_Fiction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CUSjEnPIq3",
        "outputId": "5046fcd2-769d-4128-a86b-a07193a781ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Science_Explainers'...\n",
            "remote: Enumerating objects: 1993, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 1993 (delta 102), reused 116 (delta 40), pack-reused 1791\u001b[K\n",
            "Receiving objects: 100% (1993/1993), 100.63 MiB | 6.15 MiB/s, done.\n",
            "Resolving deltas: 100% (890/890), done.\n",
            "Filtering content: 100% (16/16), 847.55 MiB | 53.13 MiB/s, done.\n",
            "Encountered 2 file(s) that should have been pointers, but weren't:\n",
            "\tdataset/article_urls.csv\n",
            "\tdataset/science_explainers_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### importing dataframes of preprocessed data\n",
        "\n",
        "dataset_ids = ['explainer', 'fiction', 'news', 'sci_paper']\n",
        "data_types = ['sw_dtm', 'dtm', 'tfidf', 'pos']\n",
        "\n",
        "for dataset in dataset_ids:\n",
        "  for d_type in data_types:\n",
        "\n",
        "    path_to_csv = f'Science_Explainers/dataset/preprocessed_data/{dataset}_{d_type}.csv'\n",
        "\n",
        "    # creates a variable name and assigns it the df\n",
        "    # e.g.,\n",
        "    # explainer_dtm_df = pd.read_csv(Science_Explainers/dataset/preprocessed_data/explainer_dtm.csv)\n",
        "    globals()[f'{dataset}_{d_type}_df'] = pd.read_csv(path_to_csv)"
      ],
      "metadata": {
        "id": "a98-w61QQwcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "mLBY_5W3mzcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "y8riYTJ_BY6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### makes all the column names UPPERCASE\n",
        "def col_names_to_uppercase(df):\n",
        "  new_columns = [name.upper() for name in df.columns]\n",
        "  df.columns = new_columns\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "5ST1_3i1qkF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_df_columns(df, list_of_titles):\n",
        "\n",
        "  # the new df with only the columns to keep\n",
        "  df_copy = df[list_of_titles]\n",
        "\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "tsbz9rQLVvPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling"
      ],
      "metadata": {
        "id": "qtSVLIAgBsIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### custom pre-processor to eliminte numbers and instances of \"_\" and \"\\\"\n",
        "def my_preprocessor(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('([0-9_\\\\\\\\])', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "t8bwIqU-zcC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_df_to_data(df):\n",
        "  return df.values.tolist()"
      ],
      "metadata": {
        "id": "q8PvsCfva8lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "Q_y7JDkmBiKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### assumes the matrix starts after the 5th column\n",
        "def sort_top_words(matrix_df, normalize = True):\n",
        "\n",
        "  # sums the values, result is a Pandas series\n",
        "  sum_values = matrix_df.sum()\n",
        "\n",
        "  if normalize:\n",
        "    # divides the sums by the number of words\n",
        "    sum_values = sum_values/len(sum_values)\n",
        "\n",
        "  sorted_dict = sum_values.sort_values(ascending = False).to_dict()\n",
        "\n",
        "  return sorted_dict"
      ],
      "metadata": {
        "id": "Ty5oAXKPcIGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function from https://github.com/dhmit/gender_novels/blob/master/gender_novels/analysis/dunning.py\n",
        "def dunn_individual_word(total_words_in_corpus_1, total_words_in_corpus_2,\n",
        "                         count_of_word_in_corpus_1,\n",
        "                         count_of_word_in_corpus_2):\n",
        "    '''\n",
        "    applies dunning log likelihood to compare individual word in two counter objects\n",
        "\n",
        "    :param word: desired word to compare\n",
        "    :param m_corpus: c.filter_by_gender('male')\n",
        "    :param f_corpus: c. filter_by_gender('female')\n",
        "    :return: log likelihoods and p value\n",
        "    >>> total_words_m_corpus = 8648489\n",
        "    >>> total_words_f_corpus = 8700765\n",
        "    >>> wordcount_female = 1000\n",
        "    >>> wordcount_male = 50\n",
        "    >>> dunn_individual_word(total_words_m_corpus,total_words_f_corpus,wordcount_male,wordcount_female)\n",
        "    -1047.8610274053995\n",
        "    '''\n",
        "    a = count_of_word_in_corpus_1\n",
        "    b = count_of_word_in_corpus_2\n",
        "    c = total_words_in_corpus_1\n",
        "    d = total_words_in_corpus_2\n",
        "\n",
        "    e1 = c * (a + b) / (c + d)\n",
        "    e2 = d * (a + b) / (c + d)\n",
        "\n",
        "    dunning_log_likelihood = 2 * (a * math.log(a / e1) + b * math.log(b / e2))\n",
        "\n",
        "    if count_of_word_in_corpus_1 * math.log(count_of_word_in_corpus_1 / e1) < 0:\n",
        "        dunning_log_likelihood = -dunning_log_likelihood\n",
        "\n",
        "    p = 1 - chi2.cdf(abs(dunning_log_likelihood),1)\n",
        "\n",
        "    return dunning_log_likelihood"
      ],
      "metadata": {
        "id": "8tn9xM0EAvGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function from https://github.com/dhmit/gender_novels/blob/master/gender_novels/analysis/dunning.py\n",
        "def dunning_total(counter1, counter2, filename_to_pickle=None):\n",
        "    '''\n",
        "    runs dunning_individual on words shared by both counter objects\n",
        "    (-) end of spectrum is words for counter_2\n",
        "    (+) end of spectrum is words for counter_1\n",
        "    the larger the magnitude of the number, the more distinctive that word is in its\n",
        "    respective counter object\n",
        "\n",
        "    use filename_to_pickle to store the result so it only has to be calculated once and can be\n",
        "    used for multiple analyses.\n",
        "\n",
        "    >>> from collections import Counter\n",
        "    >>> female_counter = Counter({'he': 1,  'she': 10, 'and': 10})\n",
        "    >>> male_counter =   Counter({'he': 10, 'she': 1,  'and': 10})\n",
        "    >>> results = dunning_total(female_counter, male_counter)\n",
        "\n",
        "    # Results is a dict that maps from terms to results\n",
        "    # Each result dict contains the dunning score...\n",
        "    >>> results['he']['dunning']\n",
        "    -8.547243830635558\n",
        "\n",
        "    # ... counts for corpora 1 and 2 as well as total count\n",
        "    >>> results['he']['count_total'], results['he']['count_corp1'], results['he']['count_corp2']\n",
        "    (11, 1, 10)\n",
        "\n",
        "    # ... and the same for frequencies\n",
        "    >>> results['he']['freq_total'], results['he']['freq_corp1'], results['he']['freq_corp2']\n",
        "    (0.2619047619047619, 0.047619047619047616, 0.47619047619047616)\n",
        "\n",
        "    :return: dict\n",
        "\n",
        "    '''\n",
        "\n",
        "    total_words_counter1 = 0\n",
        "    total_words_counter2 = 0\n",
        "\n",
        "    #get word total in respective counters\n",
        "    for word1 in counter1:\n",
        "        total_words_counter1 += counter1[word1]\n",
        "    for word2 in  counter2:\n",
        "        total_words_counter2 += counter2[word2]\n",
        "\n",
        "    #dictionary where results will be returned\n",
        "    dunning_result = {}\n",
        "    for word in counter1:\n",
        "        counter1_wordcount = counter1[word]\n",
        "        if word in counter2:\n",
        "            counter2_wordcount = counter2[word]\n",
        "\n",
        "\n",
        "            if counter1_wordcount + counter2_wordcount < 10:\n",
        "                continue\n",
        "\n",
        "            dunning_word = dunn_individual_word( total_words_counter1,  total_words_counter2,\n",
        "                                                 counter1_wordcount,counter2_wordcount)\n",
        "\n",
        "            dunning_result[word] = {\n",
        "                'dunning': dunning_word,\n",
        "                'count_total': counter1_wordcount + counter2_wordcount,\n",
        "                'count_corp1': counter1_wordcount,\n",
        "                'count_corp2': counter2_wordcount,\n",
        "                'freq_total': (counter1_wordcount + counter2_wordcount) / (total_words_counter1 +\n",
        "                                                                           total_words_counter2),\n",
        "                'freq_corp1': counter1_wordcount / total_words_counter1,\n",
        "                'freq_corp2': counter2_wordcount / total_words_counter2\n",
        "            }\n",
        "\n",
        "    return dunning_result"
      ],
      "metadata": {
        "id": "ywZlnJ8RAsYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dunning_log_likelihood_test(count_dict_1, count_dict_2):\n",
        "\n",
        "  # runs the comparison\n",
        "  dunning_comparison = dunning_total(count_dict_1, count_dict_2)\n",
        "\n",
        "  # extracting the dunning values for each word\n",
        "  dunning_values = {}\n",
        "  for term in dunning_comparison:\n",
        "    dunning_values[term] = dunning_comparison[term]['dunning']\n",
        "\n",
        "  return dunning_comparison, dunning_values"
      ],
      "metadata": {
        "id": "UGHOULPSNOSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_dunning_dict(dunning_values, length = ''):\n",
        "  # the sorted keys of the dunning_values dictionary\n",
        "  sorted_keys_desc = sorted(dunning_values, key = dunning_values.get, reverse = True)\n",
        "  sorted_keys_asc = sorted(dunning_values, key = dunning_values.get, reverse = False)\n",
        "\n",
        "  top_dunning_dict_desc = {}\n",
        "  top_dunning_dict_asc = {}\n",
        "\n",
        "  # if no dictionary length specified or specified length too long, do all\n",
        "  if length == '' or length > len(sorted_keys_desc):\n",
        "    length = len(sorted_keys_desc)\n",
        "\n",
        "  for i in range(length):\n",
        "    cur_word_desc = sorted_keys_desc[i]\n",
        "    cur_word_asc = sorted_keys_asc[i]\n",
        "\n",
        "    top_dunning_dict_desc[cur_word_desc] = dunning_values[cur_word_desc]\n",
        "    top_dunning_dict_asc[cur_word_asc] = dunning_values[cur_word_asc]\n",
        "\n",
        "  return top_dunning_dict_desc, top_dunning_dict_asc"
      ],
      "metadata": {
        "id": "CX-YBmgkDLUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "wqjsdlKlVcNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood on word counts\n",
        "\n"
      ],
      "metadata": {
        "id": "TfBYG0M0Adty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "V8LFJrHdWXau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_corpora(matrix_df_1, matrix_df_2, verbose = False, filename = ''):\n",
        "  counts_dict_1 = sort_top_words(matrix_df_1, normalize = False)\n",
        "  counts_dict_2 = sort_top_words(matrix_df_2, normalize = False)\n",
        "\n",
        "  dunn_comparison_words, dunn_value_words = run_dunning_log_likelihood_test(counts_dict_1, counts_dict_2)\n",
        "  desc_dunn_words_dict, asc_dunn_words_dict = create_top_dunning_dict(dunn_value_words, length = 30)\n",
        "\n",
        "  df_1_scores = desc_dunn_words_dict.items()\n",
        "  df_2_scores = asc_dunn_words_dict.items()\n",
        "\n",
        "  if verbose:\n",
        "    print('Corpus 1:')\n",
        "    print(tabulate(df_1_scores, headers = ['word', 'score']))\n",
        "\n",
        "    print('')\n",
        "\n",
        "    print('Corpus 2:')\n",
        "    print(tabulate(df_2_scores, headers = ['word', 'score']))\n",
        "\n",
        "  if filename != '':\n",
        "    with open(filename, 'w+') as cur_file:\n",
        "      cur_file.write(tabulate(df_1_scores, headers = ['Word', 'Score']))\n",
        "      cur_file.write('\\n')\n",
        "      cur_file.write(tabulate(df_2_scores, headers = ['Word', 'Score']))"
      ],
      "metadata": {
        "id": "mkPYxZO308UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_corpora(explainer_pos_df.iloc[:, 9:], fiction_pos_df.iloc[:, 9:], filename = 'pos_fiction.txt')"
      ],
      "metadata": {
        "id": "T5_6HoFR2HIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_corpora(explainer_pos_df.iloc[:, 9:], news_pos_df.iloc[:, 6:], filename = 'pos_news.txt')"
      ],
      "metadata": {
        "id": "eahKSyrYaBtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_corpora(explainer_pos_df.iloc[:, 9:], sci_paper_pos_df.iloc[:, 6:], filename = 'pos_sci_paper.txt')"
      ],
      "metadata": {
        "id": "RkcpL8U63fxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood on POS tags"
      ],
      "metadata": {
        "id": "Y9yXYfxHFkqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### comparing the statistical frequency of words in each corpus\n",
        "\n",
        "explainer_pure_pos_dict = sort_top_words(explainer_pos_df.iloc[:, 9:], normalize = False)\n",
        "fiction_pure_pos_dict = sort_top_words(fiction_pos_df.iloc[:, 9:], normalize = False)\n",
        "\n",
        "dunning_comparison_pos, dunning_values_pos, = run_dunning_log_likelihood_test(explainer_pure_pos_dict, fiction_pure_pos_dict)\n",
        "top_dunning_dict_pos_desc, top_dunning_dict_pos_asc = create_top_dunning_dict(dunning_values_pos, length = 15)"
      ],
      "metadata": {
        "id": "DrIvvSifF1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### POS tags associated with science explainer corpus\n",
        "print('Top 15 POS tags (science explainers):\\n')\n",
        "top_dunning_dict_pos_desc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_3dgC-5MsBv",
        "outputId": "271fe977-c18e-46d2-c898-d4785d135d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 POS tags (science explainers):\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NNS': 21760.6281917015,\n",
              " 'CD': 6613.039143529279,\n",
              " 'NNP': 5130.4600554325625,\n",
              " 'POS': 4741.982076744441,\n",
              " 'JJ': 3369.6783182861036,\n",
              " 'VBZ': 3218.556339050927,\n",
              " 'JJR': 2417.7164420526924,\n",
              " 'WDT': 2335.607703333897,\n",
              " 'IN': 1667.5106853350699,\n",
              " 'NNPS': 1604.582961501133,\n",
              " 'RBR': 1026.4570937808144,\n",
              " 'JJS': 655.310095308324,\n",
              " 'VBN': 513.2617484916518,\n",
              " 'RBS': 470.53268677068615,\n",
              " 'MD': 332.5171186622274}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### POS tags associated with short fiction corpus\n",
        "print('Top 15 POS tags (short stories):\\n')\n",
        "top_dunning_dict_pos_asc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYbcxkMM6f7",
        "outputId": "b21b492f-15a8-4d55-9643-8f5d5cc15c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 POS tags (short stories):\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PRP': -46320.77891296836,\n",
              " 'VBD': -26336.026567843386,\n",
              " 'PRP$': -13040.276514577181,\n",
              " 'RP': -1575.2704498059793,\n",
              " 'RB': -1065.6819441864463,\n",
              " 'UH': -529.9823236867871,\n",
              " 'WP': -358.9196823132338,\n",
              " 'WRB': -274.0735662347638,\n",
              " 'PDT': -246.87920259144562,\n",
              " 'CC': -189.31592856545194,\n",
              " 'FW': -85.6488511310958,\n",
              " 'VB': -22.21861838888094,\n",
              " 'WP$': -4.113230055418221,\n",
              " 'NN': -2.5493220451248817,\n",
              " 'EX': -0.5988371577817659}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}