{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithrts/Science_Explainers/blob/main/analysis/analysis_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il4s_vHhFaj1"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKJMJPjMzJj6",
        "outputId": "46d50170-ce25-475d-b3f3-4f27bf9eb073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jFpPaq5PE03"
      },
      "source": [
        "# Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CUSjEnPIq3",
        "outputId": "d777251b-2684-4f45-8a8d-73ddc50054cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Science_Explainers'...\n",
            "remote: Enumerating objects: 1912, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 1912 (delta 73), reused 54 (delta 26), pack-reused 1791\u001b[K\n",
            "Receiving objects: 100% (1912/1912), 97.86 MiB | 6.05 MiB/s, done.\n",
            "Resolving deltas: 100% (861/861), done.\n",
            "Updating files: 100% (20/20), done.\n"
          ]
        }
      ],
      "source": [
        "### cloning git repos\n",
        "\n",
        "!git clone https://github.com/faithrts/Science_Explainers\n",
        "#!git clone https://github.com/dhmit/gender_novels\n",
        "#!git clone https://github.com/faithrts/Short_Fiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "a98-w61QQwcd"
      },
      "outputs": [],
      "source": [
        "### saving datasets into dataframes\n",
        "\n",
        "explainer_df = pd.read_csv('Science_Explainers/dataset/science_explainers_dataset.csv')\n",
        "fiction_df = pd.read_csv('short_fiction_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nq_wui5tzmM"
      },
      "outputs": [],
      "source": [
        "### unzipping science explainer files\n",
        "\n",
        "!unzip Science_Explainers/dataset/science_txt_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jKFS0HPBoSQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d477870-cf75-40b8-972f-8036c2aea541"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  fiction_txt_files.zip\n",
            "replace fiction_txt_files/NEW_YORKER/ToSunland.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "### unzipping short fiction files\n",
        "\n",
        "!unzip fiction_txt_files.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLBY_5W3mzcG"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8riYTJ_BY6T"
      },
      "source": [
        "### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "NWOpRXq_FrPi"
      },
      "outputs": [],
      "source": [
        "### turns all column names to upper case\n",
        "def uppercase_columns(df):\n",
        "  columns = df.columns\n",
        "  new_columns = [column.upper() for column in columns]\n",
        "  df.columns = new_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "16ER3olQ11-L"
      },
      "outputs": [],
      "source": [
        "### counts the word count of the text and adds it as a column\n",
        "def count_text_length(df):\n",
        "  df['LENGTH'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    text = row['TEXT']\n",
        "    text_length = len(text)\n",
        "    row['LENGTH'] = text_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "LUHiQ4c5rTNp"
      },
      "outputs": [],
      "source": [
        "def load_text_content(df, path):\n",
        "\n",
        "  # adds new column to the dataframe\n",
        "  df['TEXT'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    cur_filename = row['FILENAME']\n",
        "\n",
        "    # renaming files with weird accent characters in their names\n",
        "    if 'í' in cur_filename and os.path.isfile(path + cur_filename.replace('í', 'í')):\n",
        "      os.rename(path + cur_filename.replace('í', 'í'), path + cur_filename)\n",
        "    if 'é' in cur_filename and os.path.isfile(path + cur_filename.replace('é', 'é')):\n",
        "      os.rename(path + cur_filename.replace('é', 'é'), path + cur_filename)\n",
        "\n",
        "    cur_article = codecs.open(path + cur_filename, 'r', encoding = 'utf8').read()\n",
        "\n",
        "    # saving the text in the dataframe\n",
        "    df.at[index, 'TEXT'] = cur_article\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "t8bwIqU-zcC8"
      },
      "outputs": [],
      "source": [
        "### custom pre-processor to eliminte numbers and instances of \"_\", \"\\\", and \"—\"\n",
        "def my_preprocessor(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('([0-9—_\\\\\\\\])', '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5ST1_3i1qkF7"
      },
      "outputs": [],
      "source": [
        "### makes all the column names UPPERCASE\n",
        "def col_names_to_uppercase(df):\n",
        "  new_columns = [name.upper() for name in df.columns]\n",
        "  df.columns = new_columns\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "q5se9WBOswxB"
      },
      "outputs": [],
      "source": [
        "class StemWords(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, list_of_passages):\n",
        "    # initializes the stemmer\n",
        "    snowball_stemmer = SnowballStemmer('english')\n",
        "    new_list_of_passages = []\n",
        "\n",
        "    for passage in list_of_passages:\n",
        "      # breaks the passage up into its component words\n",
        "      words = nltk.word_tokenize(passage)\n",
        "      new_words = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "      new_passage = ' '.join(new_words)\n",
        "      new_list_of_passages.append(new_passage)\n",
        "\n",
        "    return new_list_of_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tsbz9rQLVvPy"
      },
      "outputs": [],
      "source": [
        "def refine_df_columns(list_of_titles, df):\n",
        "\n",
        "  # the new df with only the columns to keep\n",
        "  df_copy = df[list_of_titles]\n",
        "\n",
        "  return df_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfB2KwOJBb9S"
      },
      "source": [
        "### Adding features to dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "6duiRZRmzfXh"
      },
      "outputs": [],
      "source": [
        "def add_dtm(df, focus_col, keep_symbols = False, keep_stopwords = False):\n",
        "\n",
        " # using CountVectorizer to make a DTM based on the words in the corpus\n",
        "  if keep_symbols:\n",
        "    vectorizer = CountVectorizer(lowercase = False, token_pattern = '[A-Z]+\\$*', min_df = 5)\n",
        "  elif keep_stopwords:\n",
        "    vectorizer = CountVectorizer(preprocessor = my_preprocessor, min_df = 5)\n",
        "  else:\n",
        "    vectorizer = CountVectorizer(preprocessor = my_preprocessor, stop_words = 'english', min_df = 5)\n",
        "\n",
        "  dtm = vectorizer.fit_transform(df[focus_col])\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # converting sparse matrix to an array of arrays\n",
        "  matrix = dtm.toarray()\n",
        "\n",
        "  # combining the DTM with the metadata (associated word)\n",
        "  DTM = pd.DataFrame(matrix, columns = words)\n",
        "\n",
        "  # attaching the DTM to the original dataframe\n",
        "  dtm_both = pd.concat([df, DTM], axis=1)\n",
        "\n",
        "  return dtm_both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rEND1QAxZncD"
      },
      "outputs": [],
      "source": [
        "def add_tf_idf(df, focus_col):\n",
        "\n",
        "  # using TfidfVectorizer to add the tf-idf values of each word to the dataframe\n",
        "  vectorizer = TfidfVectorizer(preprocessor = my_preprocessor, stop_words = 'english', min_df = 5)\n",
        "\n",
        "  tf_idf = vectorizer.fit_transform(df[focus_col])\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # converting sparse matrix to an array of arrays\n",
        "  matrix = tf_idf.toarray()\n",
        "\n",
        "  # combining the tf-idf matrix with the metadata (associated words)\n",
        "  TF_IDF = pd.DataFrame(matrix, columns = words)\n",
        "\n",
        "  # attaches the tf-idf to the original dataframe\n",
        "  tf_idf_both = pd.concat([df, TF_IDF], axis = 1)\n",
        "\n",
        "  return tf_idf_both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vfAQMFp6thOD"
      },
      "outputs": [],
      "source": [
        "### assumes the POS tags are in a column called 'POS TAGS'\n",
        "def count_pos_tags(df):\n",
        "  # concatenates all lists of POS tags into one big lists\n",
        "  all_tags = df['POS TAGS'].sum()\n",
        "\n",
        "  # counts each POS tag occurrence\n",
        "  tag_counts = Counter(all_tags)\n",
        "\n",
        "  # sorts the POS tags\n",
        "  sorted_tag_counts = sorted(tag_counts, reverse = True)\n",
        "\n",
        "  return tag_counts, sorted_tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5DP4iIDIneOB"
      },
      "outputs": [],
      "source": [
        "### assumes the text content is in a column called 'TEXT'\n",
        "def add_pos_tags(df, focus_col):\n",
        "\n",
        "  new_df = df.copy()\n",
        "\n",
        "  new_df['POS TAG TOKENS'] = ''\n",
        "  new_df['POS TAGS'] = ''\n",
        "  new_df['POS TAGS STRING'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    cur_text = row[focus_col]\n",
        "    tokenized_text = word_tokenize(cur_text)\n",
        "    POS_tags = pos_tag(tokenized_text)\n",
        "    tags_only = [tag for word,tag in POS_tags]\n",
        "\n",
        "    new_df.at[index, 'POS TAG TOKENS'] = POS_tags\n",
        "    new_df.at[index, 'POS TAGS'] = tags_only\n",
        "    new_df.at[index, 'POS TAGS STRING'] = ' '.join(tags_only)\n",
        "\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Vvg2kH934Nc8"
      },
      "outputs": [],
      "source": [
        "def create_3_dfs(df, focus_col):\n",
        "    dtm_df = add_dtm(df, focus_col).drop(columns = [focus_col])\n",
        "    tfidf_df = add_tf_idf(df, focus_col).drop(columns = [focus_col])\n",
        "\n",
        "    pos_df = add_pos_tags(df, focus_col).drop(columns = [focus_col])\n",
        "    pos_df = add_dtm(pos_df, 'POS TAGS STRING', keep_symbols = True)\n",
        "\n",
        "    return dtm_df, tfidf_df, pos_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqjsdlKlVcNO"
      },
      "source": [
        "# Loading content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ZEp9U9CJa8Sw"
      },
      "outputs": [],
      "source": [
        "### adding the text of each article as a column in the dataframe\n",
        "\n",
        "# science explainers\n",
        "explainer_df = load_text_content(explainer_df, 'science_txt_files/')\n",
        "count_text_length(explainer_df)\n",
        "\n",
        "# fiction\n",
        "fiction_df = load_text_content(fiction_df, 'fiction_txt_files/')\n",
        "count_text_length(fiction_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### adding the text of each article as a column in the dataframe\n",
        "\n",
        "# news articles\n",
        "news_df = pd.read_csv('news_dataset.csv')\n",
        "count_text_length(news_df)\n",
        "\n",
        "# scientific papers\n",
        "sci_paper_df = pd.read_csv('scientific_papers_dataset.csv')\n",
        "count_text_length(sci_paper_df)"
      ],
      "metadata": {
        "id": "fMGm3qfgSjAO"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MekxWEjm1-W"
      },
      "source": [
        "## Extending dataframes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### keeping stopwords\n",
        "\n",
        "explainer_sw_dtm_df = add_dtm(explainer_df, 'TEXT', keep_stopwords = True).drop(columns = ['TEXT'])\n",
        "fiction_sw_dtm_df = add_dtm(fiction_df, 'TEXT', keep_stopwords = True).drop(columns = ['TEXT'])\n",
        "\n",
        "news_sw_dtm_df = add_dtm(news_df, 'TEXT', keep_stopwords = True).drop(columns = ['TEXT'])\n",
        "sci_paper_sw_dtm_df = add_dtm(sci_paper_df, 'TEXT', keep_stopwords = True).drop(columns = ['TEXT'])"
      ],
      "metadata": {
        "id": "QckR14tyRkEA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1gP9Vc2bq7k"
      },
      "outputs": [],
      "source": [
        "### adding the DTM, TF-IDF, and POS tag count to the dataframes\n",
        "\n",
        "explainer_dtm_df, explainer_tfidf_df, explainer_pos_df = create_3_dfs(explainer_df, 'TEXT')\n",
        "fiction_dtm_df, fiction_tfidf_df, fiction_pos_df = create_3_dfs(fiction_df, 'TEXT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5u1mF-W4Nc8"
      },
      "outputs": [],
      "source": [
        "### adding the DTM, TF-IDF, and POS tag count to the given dataframe\n",
        "\n",
        "sci_paper_dtm_df, sci_paper_tfidf_df, sci_paper_pos_df = create_3_dfs(sci_papers_df, 'TEXT')\n",
        "news_dtm_df, news_tfidf_df, news_pos_df = create_3_dfs(news_df, 'TEXT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgFw1kjmqs-d"
      },
      "source": [
        "# Downloading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnDjmJc4Nc8"
      },
      "outputs": [],
      "source": [
        "### pickling dataframes\n",
        "\n",
        "import pickle\n",
        "\n",
        "def pickle_dataframe(name):\n",
        "\n",
        "  dfs_to_download = [f'{name}_dtm_df', f'{name}_tfidf_df', f'{name}_pos_df']\n",
        "\n",
        "  for df_name in dfs_to_download:\n",
        "    filename = df_name.split('_df')[0] + '.pkl'\n",
        "\n",
        "    with open(filename, 'wb') as cur_file:    # open a text file\n",
        "      pickle.dump(eval(df_name), cur_file)          # serialize the list\n",
        "\n",
        "    # eval(df_name).to_csv(filename, index = False, escapechar='\\\\')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def download_csv_files(df_id):\n",
        "  ### downloading csv of dataframes\n",
        "\n",
        "  suffices = ['_dtm_df', '_tfidf_df', '_pos_df']\n",
        "  dfs_to_download = [df_id + suffix for suffix in suffices]\n",
        "\n",
        "  for df_name in dfs_to_download:\n",
        "    filename = df_name.split('_df')[0] + '.csv'\n",
        "    eval(df_name).to_csv(filename, index = False, escapechar='\\\\')\n",
        "    files.download(filename)"
      ],
      "metadata": {
        "id": "tJ8wijyGhnoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def download_single_csv(df_name):\n",
        "\n",
        "  filename = df_name.split('_df')[0] + '.csv'\n",
        "  eval(df_name).to_csv(filename, index = False, escapechar='\\\\')\n",
        "  files.download(filename)"
      ],
      "metadata": {
        "id": "frAAdB22RqIT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pa6h1ajLqt4V",
        "outputId": "9ee070a7-2a6e-41c9-c895-7e367bd6f182"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7a3aaaac-89d6-48ae-b004-f6d188f02242\", \"explainer_dtm.csv\", 19185408)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_97518906-16d6-4979-af14-d12c6316290d\", \"fiction_dtm.csv\", 31566363)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_edb4a7be-9f54-496c-b2ad-1cb925fcaec1\", \"explainer_tfidf.csv\", 43624911)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7067dc75-ca23-420a-9552-87e9cade9748\", \"fiction_tfidf.csv\", 73171899)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f0a2df88-1cbc-4c9b-bcf7-1e7fc3dfc8d9\", \"explainer_pos.csv\", 35840577)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8d3ed536-612a-4270-b7eb-d44b80113342\", \"fiction_pos.csv\", 87460109)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### downloading csv of dataframes\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "dfs_to_download = ['explainer_dtm_df', 'fiction_dtm_df',\n",
        "                   'explainer_tfidf_df', 'fiction_tfidf_df',\n",
        "                   'explainer_pos_df', 'fiction_pos_df']\n",
        "\n",
        "for df_name in dfs_to_download:\n",
        "  filename = df_name.split('_df')[0] + '.csv'\n",
        "  eval(df_name).to_csv(filename, index = False, escapechar='\\\\')\n",
        "  files.download(filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_single_csv('sci_paper_sw_dtm_df')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OBbdpWQ_SaTu",
        "outputId": "1f3549c5-8278-4a81-dbe3-7347350a0a4f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_514591dd-83bc-45d4-b720-48947b0aba86\", \"sci_paper_sw_dtm.csv\", 53247700)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_single_csv('news_sw_dtm_df')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FShM1mrtSgYP",
        "outputId": "f75178e4-7508-4b95-da0c-bcc43be5900f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dc64944b-797d-4d1d-a50b-eeccf46dbc74\", \"news_sw_dtm.csv\", 28728909)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}