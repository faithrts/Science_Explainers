{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithrts/Science_Explainers/blob/main/analysis/analysis_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il4s_vHhFaj1"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKJMJPjMzJj6",
        "outputId": "c9d42988-ff25-436e-a55e-89759e450217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# to download files\n",
        "# from google.colab import files\n",
        "\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jFpPaq5PE03"
      },
      "source": [
        "# Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CUSjEnPIq3",
        "outputId": "d777251b-2684-4f45-8a8d-73ddc50054cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Science_Explainers'...\n",
            "remote: Enumerating objects: 1912, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 1912 (delta 73), reused 54 (delta 26), pack-reused 1791\u001b[K\n",
            "Receiving objects: 100% (1912/1912), 97.86 MiB | 6.05 MiB/s, done.\n",
            "Resolving deltas: 100% (861/861), done.\n",
            "Updating files: 100% (20/20), done.\n"
          ]
        }
      ],
      "source": [
        "### cloning git repos\n",
        "\n",
        "!git clone https://github.com/faithrts/Science_Explainers\n",
        "#!git clone https://github.com/dhmit/gender_novels\n",
        "#!git clone https://github.com/faithrts/Short_Fiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a98-w61QQwcd",
        "outputId": "ac631886-0faa-4bf5-b737-5a03239c0e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'short_fiction_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1817973cc03a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexplainer_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Science_Explainers/dataset/science_explainers_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'short_fiction_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short_fiction_dataset.csv'"
          ]
        }
      ],
      "source": [
        "### saving datasets into dataframes\n",
        "\n",
        "explainer_df = pd.read_csv('Science_Explainers/dataset/science_explainers_dataset.csv')\n",
        "fiction_df = pd.read_csv('short_fiction_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nq_wui5tzmM"
      },
      "outputs": [],
      "source": [
        "### unzipping science explainer files\n",
        "\n",
        "!unzip Science_Explainers/dataset/science_txt_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKFS0HPBoSQF"
      },
      "outputs": [],
      "source": [
        "### unzipping short fiction files\n",
        "\n",
        "!unzip fiction_txt_files.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLBY_5W3mzcG"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8riYTJ_BY6T"
      },
      "source": [
        "### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NWOpRXq_FrPi"
      },
      "outputs": [],
      "source": [
        "### turns all column names to upper case\n",
        "def uppercase_columns(df):\n",
        "  columns = df.columns\n",
        "  new_columns = [column.upper() for column in columns]\n",
        "  df.columns = new_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "16ER3olQ11-L"
      },
      "outputs": [],
      "source": [
        "### counts the word count of the text and adds it as a column\n",
        "def count_text_length(df):\n",
        "  df['LENGTH'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    text = row['TEXT']\n",
        "    text_length = len(text)\n",
        "    row['LENGTH'] = text_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LUHiQ4c5rTNp"
      },
      "outputs": [],
      "source": [
        "def load_text_content(df, path):\n",
        "\n",
        "  # adds new column to the dataframe\n",
        "  df['TEXT'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    cur_filename = row['FILENAME']\n",
        "\n",
        "    # renaming files with weird accent characters in their names\n",
        "    if 'í' in cur_filename and os.path.isfile(path + cur_filename.replace('í', 'í')):\n",
        "      os.rename(path + cur_filename.replace('í', 'í'), path + cur_filename)\n",
        "    if 'é' in cur_filename and os.path.isfile(path + cur_filename.replace('é', 'é')):\n",
        "      os.rename(path + cur_filename.replace('é', 'é'), path + cur_filename)\n",
        "\n",
        "    cur_article = codecs.open(path + cur_filename, 'r', encoding = 'utf8').read()\n",
        "\n",
        "    # saving the text in the dataframe\n",
        "    df.at[index, 'TEXT'] = cur_article\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t8bwIqU-zcC8"
      },
      "outputs": [],
      "source": [
        "### custom pre-processor to eliminte numbers and instances of \"_\", \"\\\", and \"—\"\n",
        "def my_preprocessor(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('([0-9—_\\\\\\\\])', '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5ST1_3i1qkF7"
      },
      "outputs": [],
      "source": [
        "### makes all the column names UPPERCASE\n",
        "def col_names_to_uppercase(df):\n",
        "  new_columns = [name.upper() for name in df.columns]\n",
        "  df.columns = new_columns\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q5se9WBOswxB"
      },
      "outputs": [],
      "source": [
        "class StemWords(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, list_of_passages):\n",
        "    # initializes the stemmer\n",
        "    snowball_stemmer = SnowballStemmer('english')\n",
        "    new_list_of_passages = []\n",
        "\n",
        "    for passage in list_of_passages:\n",
        "      # breaks the passage up into its component words\n",
        "      words = nltk.word_tokenize(passage)\n",
        "      new_words = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "      new_passage = ' '.join(new_words)\n",
        "      new_list_of_passages.append(new_passage)\n",
        "\n",
        "    return new_list_of_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tsbz9rQLVvPy"
      },
      "outputs": [],
      "source": [
        "def refine_df_columns(list_of_titles, df):\n",
        "\n",
        "  # the new df with only the columns to keep\n",
        "  df_copy = df[list_of_titles]\n",
        "\n",
        "  return df_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfB2KwOJBb9S"
      },
      "source": [
        "### Adding features to dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6duiRZRmzfXh"
      },
      "outputs": [],
      "source": [
        "def add_dtm(df, focus_col, keep_symbols = False):\n",
        "\n",
        " # using CountVectorizer to make a DTM based on the words in the corpus\n",
        "  if keep_symbols:\n",
        "    vectorizer = CountVectorizer(lowercase = False, token_pattern = '[A-Z]+\\$*', min_df = 5)\n",
        "  else:\n",
        "    vectorizer = CountVectorizer(preprocessor = my_preprocessor, stop_words = 'english', min_df = 5)\n",
        "\n",
        "  dtm = vectorizer.fit_transform(df[focus_col])\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # converting sparse matrix to an array of arrays\n",
        "  matrix = dtm.toarray()\n",
        "\n",
        "  # combining the DTM with the metadata (associated word)\n",
        "  DTM = pd.DataFrame(matrix, columns = words)\n",
        "\n",
        "  # attaching the DTM to the original dataframe\n",
        "  dtm_both = pd.concat([df, DTM], axis=1)\n",
        "\n",
        "  return dtm_both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rEND1QAxZncD"
      },
      "outputs": [],
      "source": [
        "def add_tf_idf(df, focus_col):\n",
        "\n",
        "  # using TfidfVectorizer to add the tf-idf values of each word to the dataframe\n",
        "  vectorizer = TfidfVectorizer(preprocessor = my_preprocessor, stop_words = 'english', min_df = 5)\n",
        "\n",
        "  tf_idf = vectorizer.fit_transform(df[focus_col])\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # converting sparse matrix to an array of arrays\n",
        "  matrix = tf_idf.toarray()\n",
        "\n",
        "  # combining the tf-idf matrix with the metadata (associated words)\n",
        "  TF_IDF = pd.DataFrame(matrix, columns = words)\n",
        "\n",
        "  # attaches the tf-idf to the original dataframe\n",
        "  tf_idf_both = pd.concat([df, TF_IDF], axis = 1)\n",
        "\n",
        "  return tf_idf_both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vfAQMFp6thOD"
      },
      "outputs": [],
      "source": [
        "### assumes the POS tags are in a column called 'POS TAGS'\n",
        "def count_pos_tags(df):\n",
        "  # concatenates all lists of POS tags into one big lists\n",
        "  all_tags = df['POS TAGS'].sum()\n",
        "\n",
        "  # counts each POS tag occurrence\n",
        "  tag_counts = Counter(all_tags)\n",
        "\n",
        "  # sorts the POS tags\n",
        "  sorted_tag_counts = sorted(tag_counts, reverse = True)\n",
        "\n",
        "  return tag_counts, sorted_tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5DP4iIDIneOB"
      },
      "outputs": [],
      "source": [
        "### assumes the text content is in a column called 'TEXT'\n",
        "def add_pos_tags(df, focus_col):\n",
        "\n",
        "  new_df = df.copy()\n",
        "\n",
        "  new_df['POS TAG TOKENS'] = ''\n",
        "  new_df['POS TAGS'] = ''\n",
        "  new_df['POS TAGS STRING'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    cur_text = row[focus_col]\n",
        "    tokenized_text = word_tokenize(cur_text)\n",
        "    POS_tags = pos_tag(tokenized_text)\n",
        "    tags_only = [tag for word,tag in POS_tags]\n",
        "\n",
        "    new_df.at[index, 'POS TAG TOKENS'] = POS_tags\n",
        "    new_df.at[index, 'POS TAGS'] = tags_only\n",
        "    new_df.at[index, 'POS TAGS STRING'] = ' '.join(tags_only)\n",
        "\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vvg2kH934Nc8"
      },
      "outputs": [],
      "source": [
        "def create_3_dfs(df, focus_col):\n",
        "    dtm_df = add_dtm(df, focus_col).drop(columns = [focus_col])\n",
        "    tfidf_df = add_tf_idf(df, focus_col).drop(columns = [focus_col])\n",
        "\n",
        "    pos_df = add_pos_tags(df, focus_col).drop(columns = [focus_col])\n",
        "    pos_df = add_dtm(pos_df, 'POS TAGS STRING', keep_symbols = True)\n",
        "\n",
        "    return dtm_df, tfidf_df, pos_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqjsdlKlVcNO"
      },
      "source": [
        "# Loading content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FQs-QadD4Nc8"
      },
      "outputs": [],
      "source": [
        "cur_df = pd.read_csv('scientific_papers_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEp9U9CJa8Sw"
      },
      "outputs": [],
      "source": [
        "### adding the text of each article as a column in the dataframe\n",
        "\n",
        "# science explainers\n",
        "explainer_df = load_text_content(explainer_df, 'science_txt_files/')\n",
        "count_text_length(explainer_df)\n",
        "\n",
        "# fiction\n",
        "fiction_df = load_text_content(fiction_df, 'fiction_txt_files/')\n",
        "count_text_length(fiction_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MekxWEjm1-W"
      },
      "source": [
        "## Extending dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1gP9Vc2bq7k"
      },
      "outputs": [],
      "source": [
        "### adding the DTM, TF-IDF, and POS tag count to the dataframes\n",
        "\n",
        "explainer_dtm_df, explainer_tfidf_df, explainer_pos_df = create_3_dfs(explainer_df, 'TEXT')\n",
        "fiction_dtm_df, fiction_tfidf_df, fiction_pos_df = create_3_dfs(fiction_df, 'TEXT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "E5u1mF-W4Nc8"
      },
      "outputs": [],
      "source": [
        "### adding the DTM, TF-IDF, and POS tag count to the given dataframe\n",
        "\n",
        "cur_dtm_df, cur_tfidf_df, cur_pos_df = create_3_dfs(cur_df, 'TEXT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgFw1kjmqs-d"
      },
      "source": [
        "# Downloading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cGnDjmJc4Nc8"
      },
      "outputs": [],
      "source": [
        "### pickling dataframes\n",
        "\n",
        "import pickle\n",
        "\n",
        "dfs_to_download = ['cur_dtm_df', 'cur_tfidf_df', 'cur_pos_df']\n",
        "\n",
        "for df_name in dfs_to_download:\n",
        "  filename = df_name.split('_df')[0] + '.pkl'\n",
        "\n",
        "  with open(filename, 'wb') as cur_file:  # open a text file\n",
        "    pickle.dump(df_name, cur_file) # serialize the list\n",
        "\n",
        "  # eval(df_name).to_csv(filename, index = False, escapechar='\\\\')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pa6h1ajLqt4V",
        "outputId": "9ee070a7-2a6e-41c9-c895-7e367bd6f182"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7a3aaaac-89d6-48ae-b004-f6d188f02242\", \"explainer_dtm.csv\", 19185408)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_97518906-16d6-4979-af14-d12c6316290d\", \"fiction_dtm.csv\", 31566363)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_edb4a7be-9f54-496c-b2ad-1cb925fcaec1\", \"explainer_tfidf.csv\", 43624911)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7067dc75-ca23-420a-9552-87e9cade9748\", \"fiction_tfidf.csv\", 73171899)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f0a2df88-1cbc-4c9b-bcf7-1e7fc3dfc8d9\", \"explainer_pos.csv\", 35840577)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8d3ed536-612a-4270-b7eb-d44b80113342\", \"fiction_pos.csv\", 87460109)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### downloading csv of dataframes\n",
        "\n",
        "dfs_to_download = ['explainer_dtm_df', 'fiction_dtm_df',\n",
        "                   'explainer_tfidf_df', 'fiction_tfidf_df',\n",
        "                   'explainer_pos_df', 'fiction_pos_df']\n",
        "\n",
        "for df_name in dfs_to_download:\n",
        "  filename = df_name.split('_df')[0] + '.csv'\n",
        "  eval(df_name).to_csv(filename, index = False, escapechar='\\\\')\n",
        "  files.download(filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}