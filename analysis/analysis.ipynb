{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithrts/Science_Explainers/blob/main/analysis/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Il4s_vHhFaj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nKJMJPjMzJj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02932348-ed4d-43c9-d0d6-a0291c1b1170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# to download files\n",
        "from google.colab import files\n",
        "\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "import codecs\n",
        "\n",
        "# for word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# scipy\n",
        "from scipy.stats import chi2  ## for stats\n",
        "from scipy import spatial     ## for cosine similarity\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# silencing the warnings\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing databases"
      ],
      "metadata": {
        "id": "1jFpPaq5PE03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### cloning git repos\n",
        "\n",
        "!git clone https://github.com/faithrts/Science_Explainers\n",
        "#!git clone https://github.com/dhmit/gender_novels\n",
        "#!git clone https://github.com/faithrts/Short_Fiction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CUSjEnPIq3",
        "outputId": "4879ad58-3268-4dcd-cb9c-e99a4dd49b5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Science_Explainers'...\n",
            "remote: Enumerating objects: 1791, done.\u001b[K\n",
            "remote: Counting objects: 100% (667/667), done.\u001b[K\n",
            "remote: Compressing objects: 100% (645/645), done.\u001b[K\n",
            "remote: Total 1791 (delta 39), reused 66 (delta 22), pack-reused 1124\u001b[K\n",
            "Receiving objects: 100% (1791/1791), 111.40 MiB | 6.70 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "Updating files: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### importing dataframes:\n",
        "# [explainer/fiction]_dtm_df\n",
        "# [explainer/fiction]_tfidf_df\n",
        "# [explainer/fiction]_pos_df\n",
        "\n",
        "explainer_dtm_df = pd.read_csv('Science_Explainers/analysis/explainer_dtm.csv')\n",
        "explainer_tfidf_df = pd.read_csv('Science_Explainers/analysis/explainer_tfidf.csv')\n",
        "explainer_pos_df = pd.read_csv('Science_Explainers/analysis/explainer_pos.csv')\n",
        "\n",
        "fiction_dtm_df = pd.read_csv('Science_Explainers/analysis/fiction_dtm.csv')\n",
        "fiction_tfidf_df = pd.read_csv('Science_Explainers/analysis/fiction_tfidf.csv')\n",
        "fiction_pos_df = pd.read_csv('Science_Explainers/analysis/fiction_pos.csv')"
      ],
      "metadata": {
        "id": "a98-w61QQwcd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "mLBY_5W3mzcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "y8riYTJ_BY6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### makes all the column names UPPERCASE\n",
        "def col_names_to_uppercase(df):\n",
        "  new_columns = [name.upper() for name in df.columns]\n",
        "  df.columns = new_columns\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "5ST1_3i1qkF7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StemWords(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, list_of_passages):\n",
        "    # initializes the stemmer\n",
        "    snowball_stemmer = SnowballStemmer('english')\n",
        "    new_list_of_passages = []\n",
        "\n",
        "    for passage in list_of_passages:\n",
        "      # breaks the passage up into its component words\n",
        "      words = nltk.word_tokenize(passage)\n",
        "      new_words = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "      new_passage = ' '.join(new_words)\n",
        "      new_list_of_passages.append(new_passage)\n",
        "\n",
        "    return new_list_of_passages"
      ],
      "metadata": {
        "id": "q5se9WBOswxB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_df_columns(df, list_of_titles):\n",
        "\n",
        "  # the new df with only the columns to keep\n",
        "  df_copy = df[list_of_titles]\n",
        "\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "tsbz9rQLVvPy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling"
      ],
      "metadata": {
        "id": "qtSVLIAgBsIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### custom pre-processor to eliminte numbers and instances of \"_\", \"\\\", and \"—\"\n",
        "def my_preprocessor(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('([0-9—_\\\\\\\\])', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "t8bwIqU-zcC8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_df_to_data(df):\n",
        "  return df.values.tolist()"
      ],
      "metadata": {
        "id": "q8PvsCfva8lw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_log_reg_l2(x_train, y_train, max_iterations = 5000):\n",
        "\n",
        "  # logistic regression with l2 regularization\n",
        "  model = LogisticRegression(penalty = 'l2', max_iter = max_iterations)\n",
        "\n",
        "  # fitting the model\n",
        "  model = model.fit(x_train, y_train)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hZOkiLmCuARG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_lda(x_train, topics = 10, max_iterations = 5000):\n",
        "\n",
        "  # logistic regression with l2 regularization\n",
        "  model = LatentDirichletAllocation(n_components = topics, random_state = 11)\n",
        "\n",
        "  # fitting the model\n",
        "  model = model.fit_transform(x_train)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "tzl4oBjCZNaD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_predictive_model(x_test, y_test, model):\n",
        "  # predicted labels\n",
        "  y_pred = model.predict(x_test)\n",
        "\n",
        "  # gets performance in terms of precision, recall, accuracy, etc.\n",
        "  report = classification_report(y_test, y_pred, target_names = ['e', 'f'], output_dict = True)\n",
        "  accuracy = round(report.get('accuracy') * 100, 2)\n",
        "\n",
        "  comparison = y_pred == y_test\n",
        "  incorrect_indices = [index for index, value in enumerate(comparison) if value == False]\n",
        "\n",
        "  return report, incorrect_indices"
      ],
      "metadata": {
        "id": "r7G0r_tXc8xQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_helper(explainer_feature_df, fiction_feature_df, ran = 11):\n",
        "\n",
        "  # adds the file identity to the end of each datapoint for later identification of miscategorized files\n",
        "  if 'FILENAME' not in explainer_feature_df.columns:\n",
        "    explainer_feature_df['FILENAME'] = explainer_dtm_df['FILENAME']\n",
        "  if 'FILENAME' not in fiction_feature_df.columns:\n",
        "    fiction_feature_df['FILENAME'] = fiction_dtm_df['FILENAME']\n",
        "\n",
        "  # converts dataframes into lists of feature values\n",
        "  explainer_data = convert_df_to_data(explainer_feature_df)\n",
        "  fiction_data = convert_df_to_data(fiction_feature_df)\n",
        "\n",
        "  # combines the explainer and fiction data\n",
        "  total_data = explainer_data + fiction_data\n",
        "  # creates target labels for the data\n",
        "  total_labels = ['e'] * len(explainer_data) + ['f'] * len(fiction_data)\n",
        "\n",
        "  # splits the data into training and testing (random state to keep results consistent across re-runs)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(total_data, total_labels, random_state = ran)\n",
        "\n",
        "  # retrieves the filenames for later identification (removes from data before input to model)\n",
        "  filenames_train = [datapoint.pop() for datapoint in x_train]\n",
        "  filenames_test = [datapoint.pop() for datapoint in x_test]\n",
        "\n",
        "  return x_train, x_test, y_train, y_test, filenames_train, filenames_test"
      ],
      "metadata": {
        "id": "VCuoe5GANVm3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_k_cross_validation(explainer_feature_df, fiction_feature_df, k = 5, ran = 11, model = ''):\n",
        "\n",
        "  accuracies = []\n",
        "  miscategorized_files = []\n",
        "  closest_vectors = []\n",
        "  sim_values = []\n",
        "\n",
        "  # if the filenames aren't in the feature dataframes, adds them so we can extract at the miscategorized files\n",
        "  if 'FILENAME' not in explainer_feature_df.columns:\n",
        "    explainer_feature_df['FILENAME'] = explainer_dtm_df['FILENAME']\n",
        "  if 'FILENAME' not in fiction_feature_df.columns:\n",
        "    fiction_feature_df['FILENAME'] = fiction_dtm_df['FILENAME']\n",
        "\n",
        "  # shuffles the dataframes\n",
        "  explainer_feature_shuffled_df = explainer_feature_df.sample(frac = 1, random_state = 22)\n",
        "  fiction_feature_shuffled_df = fiction_feature_df.sample(frac = 1, random_state = 22)\n",
        "\n",
        "  # converts dataframes into lists of feature values\n",
        "  explainer_data = convert_df_to_data(explainer_feature_df)\n",
        "  fiction_data = convert_df_to_data(fiction_feature_df)\n",
        "\n",
        "  # indices\n",
        "  partition_size = int(len(explainer_feature_shuffled_df) / (k * 2))\n",
        "\n",
        "  start = 0\n",
        "  end = partition_size\n",
        "  # for folds 0 to k - 1, sets partition i as the test set\n",
        "  for i in range(k):\n",
        "\n",
        "    # training data\n",
        "    x_train_explainer = explainer_data[0:start] + explainer_data[end:-1]\n",
        "    x_train_fiction = fiction_data[0:start] + fiction_data[end:-1]\n",
        "\n",
        "    # testing data\n",
        "    x_test_explainer = explainer_data[start:end]\n",
        "    x_test_fiction = fiction_data[start:end]\n",
        "\n",
        "    # combines the explainer and fiction data\n",
        "    x_train = x_train_explainer + x_train_fiction\n",
        "    x_test = x_test_explainer + x_test_fiction\n",
        "\n",
        "    # retrieves the filenames for later identification of miscategorized texts\n",
        "    filenames_train = [datapoint[-1] for datapoint in x_train]\n",
        "    filenames_test = [datapoint[-1] for datapoint in x_test]\n",
        "\n",
        "    # I do this instead of pop as for some reason, pop was modifying the original data\n",
        "    x_train = [datapoint[:-1] for datapoint in x_train]\n",
        "    x_test = [datapoint[:-1] for datapoint in x_test]\n",
        "\n",
        "    y_train = ['e'] * len(x_train_explainer) + ['f'] * len(x_train_fiction)\n",
        "    y_test =  ['e'] * len(x_test_explainer) + ['f'] * len(x_test_explainer)\n",
        "\n",
        "\n",
        "    ### trains and tests the model\n",
        "\n",
        "    if model == '':\n",
        "      # uses default model\n",
        "      model = LogisticRegression(penalty = 'l2', max_iter = 5000)\n",
        "\n",
        "    model = model.fit(x_train, y_train)\n",
        "\n",
        "    # performance report\n",
        "    report, incorrect_indices = test_predictive_model(x_test, y_test, model)\n",
        "    accuracy = round(report.get('accuracy') * 100, 2)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    ### finding the miscategorized files\n",
        "\n",
        "    # miscategorized filenames\n",
        "    miscat_explainer = [filenames_test[i] for i in incorrect_indices if i < int(len(y_test)/2)]\n",
        "    miscat_fiction = [filenames_test[i] for i in incorrect_indices if i >= int(len(y_test)/2)]\n",
        "\n",
        "    # miscategorized vector values\n",
        "    miscat_explainer_vectors = [x_test[i] for i in incorrect_indices if i < int(len(y_test)/2)]\n",
        "    miscat_fiction_vectors = [x_test[i] for i in incorrect_indices if i >= int(len(y_test)/2)]\n",
        "\n",
        "    # the indices of the closest vectors and their cosine similarity\n",
        "    closest_vector_indices, closest_sim = find_closest_vectors_between_lists(miscat_explainer_vectors, miscat_fiction_vectors)\n",
        "\n",
        "    if len(closest_vector_indices) == 2:\n",
        "      closest_vectors.append([miscat_explainer[closest_vector_indices[0]], miscat_fiction[closest_vector_indices[1]]])\n",
        "    else:\n",
        "      closest_vectors.append([])\n",
        "\n",
        "    sim_values.append(closest_sim)\n",
        "    miscategorized_files.append([miscat_explainer, miscat_fiction])\n",
        "\n",
        "    ### changing slice of data for new partitions between training and testing data\n",
        "\n",
        "    start += partition_size\n",
        "    end += partition_size\n",
        "\n",
        "  return accuracies, miscategorized_files, closest_vectors, sim_values"
      ],
      "metadata": {
        "id": "OdHKRe6WTdX5"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OLD_POP_PROBLEM_run_k_cross_validation(explainer_feature_df, fiction_feature_df, k = 5, ran = 11, model = ''):\n",
        "\n",
        "  accuracies = []\n",
        "  miscategorized_files = []\n",
        "\n",
        "  # if the filenames aren't in the feature dataframes, adds them so we can extract at the miscategorized files\n",
        "  if 'FILENAME' not in explainer_feature_df.columns:\n",
        "    explainer_feature_df['FILENAME'] = explainer_dtm_df['FILENAME']\n",
        "  if 'FILENAME' not in fiction_feature_df.columns:\n",
        "    fiction_feature_df['FILENAME'] = fiction_dtm_df['FILENAME']\n",
        "\n",
        "  # shuffles the dataframes\n",
        "  explainer_feature_shuffled_df = explainer_feature_df.sample(frac = 1, random_state = 22)\n",
        "  fiction_feature_shuffled_df = fiction_feature_df.sample(frac = 1, random_state = 22)\n",
        "\n",
        "  # converts dataframes into lists of feature values\n",
        "  explainer_data = convert_df_to_data(explainer_feature_df)\n",
        "  fiction_data = convert_df_to_data(fiction_feature_df)\n",
        "\n",
        "  # indices\n",
        "  partition_size = int(len(explainer_feature_shuffled_df) / (k * 2))\n",
        "\n",
        "  start = 0\n",
        "  end = partition_size\n",
        "  # for folds 0 to k - 1, sets partition i as the test set\n",
        "  for i in range(k):\n",
        "\n",
        "    explainer_data = copy.deepcopy(explainer_data)\n",
        "    fiction_data = copy.deepcopy(fiction_data)\n",
        "\n",
        "    # training data\n",
        "    x_train_explainer = explainer_data[0:start] + explainer_data[end:-1]\n",
        "    x_train_fiction = fiction_data[0:start] + fiction_data[end:-1]\n",
        "\n",
        "    # testing data\n",
        "    x_test_explainer = explainer_data[start:end]\n",
        "    x_test_fiction = fiction_data[start:end]\n",
        "\n",
        "    # combines the explainer and fiction data\n",
        "    x_train = x_train_explainer + x_train_fiction\n",
        "    x_test = x_test_explainer + x_test_fiction\n",
        "\n",
        "    # run this and this number will go down each time\n",
        "    print(len(x_train[5]))\n",
        "\n",
        "    # retrieves the filenames for later identification of miscategorized texts\n",
        "    filenames_train = [datapoint.pop() for datapoint in x_train]\n",
        "    filenames_test = [datapoint.pop() for datapoint in x_test]\n",
        "\n",
        "    y_train = ['e'] * len(x_train_explainer) + ['f'] * len(x_train_fiction)\n",
        "    y_test =  ['e'] * len(x_test_explainer) + ['f'] * len(x_test_explainer)\n",
        "\n",
        "    if model == '':\n",
        "      # uses default model\n",
        "      model = LogisticRegression(penalty = 'l2', max_iter = 5000)\n",
        "\n",
        "    model = model.fit(x_train, y_train)\n",
        "\n",
        "    # performance report\n",
        "    report, incorrect_indices = test_predictive_model(x_test, y_test, model)\n",
        "    accuracy = round(report.get('accuracy') * 100, 2)\n",
        "\n",
        "    '''\n",
        "    print(i)\n",
        "    print(incorrect_indices)\n",
        "    print(filenames_test)\n",
        "    print('')\n",
        "    '''\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    miscategorized_files.append([filenames_test[i] for i in incorrect_indices])\n",
        "\n",
        "    start += partition_size\n",
        "    end += partition_size\n",
        "\n",
        "  print(len(explainer_data[0]))\n",
        "  return accuracies, miscategorized_files"
      ],
      "metadata": {
        "id": "r_NT9uAEuJ1H"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "Q_y7JDkmBiKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### assumes the matrix starts after the 5th column\n",
        "def sort_top_words(matrix_df, normalize = True):\n",
        "\n",
        "  # sums the values, result is a Pandas series\n",
        "  sum_values = matrix_df.sum()\n",
        "\n",
        "  if normalize:\n",
        "    # divides the sums by the number of words\n",
        "    sum_values = sum_values/len(sum_values)\n",
        "\n",
        "  sorted_dict = sum_values.sort_values(ascending = False).to_dict()\n",
        "\n",
        "  return sorted_dict"
      ],
      "metadata": {
        "id": "Ty5oAXKPcIGj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function from https://github.com/dhmit/gender_novels/blob/master/gender_novels/analysis/dunning.py\n",
        "def dunn_individual_word(total_words_in_corpus_1, total_words_in_corpus_2,\n",
        "                         count_of_word_in_corpus_1,\n",
        "                         count_of_word_in_corpus_2):\n",
        "    '''\n",
        "    applies dunning log likelihood to compare individual word in two counter objects\n",
        "\n",
        "    :param word: desired word to compare\n",
        "    :param m_corpus: c.filter_by_gender('male')\n",
        "    :param f_corpus: c. filter_by_gender('female')\n",
        "    :return: log likelihoods and p value\n",
        "    >>> total_words_m_corpus = 8648489\n",
        "    >>> total_words_f_corpus = 8700765\n",
        "    >>> wordcount_female = 1000\n",
        "    >>> wordcount_male = 50\n",
        "    >>> dunn_individual_word(total_words_m_corpus,total_words_f_corpus,wordcount_male,wordcount_female)\n",
        "    -1047.8610274053995\n",
        "    '''\n",
        "    a = count_of_word_in_corpus_1\n",
        "    b = count_of_word_in_corpus_2\n",
        "    c = total_words_in_corpus_1\n",
        "    d = total_words_in_corpus_2\n",
        "\n",
        "    e1 = c * (a + b) / (c + d)\n",
        "    e2 = d * (a + b) / (c + d)\n",
        "\n",
        "    dunning_log_likelihood = 2 * (a * math.log(a / e1) + b * math.log(b / e2))\n",
        "\n",
        "    if count_of_word_in_corpus_1 * math.log(count_of_word_in_corpus_1 / e1) < 0:\n",
        "        dunning_log_likelihood = -dunning_log_likelihood\n",
        "\n",
        "    p = 1 - chi2.cdf(abs(dunning_log_likelihood),1)\n",
        "\n",
        "    return dunning_log_likelihood"
      ],
      "metadata": {
        "id": "8tn9xM0EAvGL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function from https://github.com/dhmit/gender_novels/blob/master/gender_novels/analysis/dunning.py\n",
        "def dunning_total(counter1, counter2, filename_to_pickle=None):\n",
        "    '''\n",
        "    runs dunning_individual on words shared by both counter objects\n",
        "    (-) end of spectrum is words for counter_2\n",
        "    (+) end of spectrum is words for counter_1\n",
        "    the larger the magnitude of the number, the more distinctive that word is in its\n",
        "    respective counter object\n",
        "\n",
        "    use filename_to_pickle to store the result so it only has to be calculated once and can be\n",
        "    used for multiple analyses.\n",
        "\n",
        "    >>> from collections import Counter\n",
        "    >>> female_counter = Counter({'he': 1,  'she': 10, 'and': 10})\n",
        "    >>> male_counter =   Counter({'he': 10, 'she': 1,  'and': 10})\n",
        "    >>> results = dunning_total(female_counter, male_counter)\n",
        "\n",
        "    # Results is a dict that maps from terms to results\n",
        "    # Each result dict contains the dunning score...\n",
        "    >>> results['he']['dunning']\n",
        "    -8.547243830635558\n",
        "\n",
        "    # ... counts for corpora 1 and 2 as well as total count\n",
        "    >>> results['he']['count_total'], results['he']['count_corp1'], results['he']['count_corp2']\n",
        "    (11, 1, 10)\n",
        "\n",
        "    # ... and the same for frequencies\n",
        "    >>> results['he']['freq_total'], results['he']['freq_corp1'], results['he']['freq_corp2']\n",
        "    (0.2619047619047619, 0.047619047619047616, 0.47619047619047616)\n",
        "\n",
        "    :return: dict\n",
        "\n",
        "    '''\n",
        "\n",
        "    total_words_counter1 = 0\n",
        "    total_words_counter2 = 0\n",
        "\n",
        "    #get word total in respective counters\n",
        "    for word1 in counter1:\n",
        "        total_words_counter1 += counter1[word1]\n",
        "    for word2 in  counter2:\n",
        "        total_words_counter2 += counter2[word2]\n",
        "\n",
        "    #dictionary where results will be returned\n",
        "    dunning_result = {}\n",
        "    for word in counter1:\n",
        "        counter1_wordcount = counter1[word]\n",
        "        if word in counter2:\n",
        "            counter2_wordcount = counter2[word]\n",
        "\n",
        "\n",
        "            if counter1_wordcount + counter2_wordcount < 10:\n",
        "                continue\n",
        "\n",
        "            dunning_word = dunn_individual_word( total_words_counter1,  total_words_counter2,\n",
        "                                                 counter1_wordcount,counter2_wordcount)\n",
        "\n",
        "            dunning_result[word] = {\n",
        "                'dunning': dunning_word,\n",
        "                'count_total': counter1_wordcount + counter2_wordcount,\n",
        "                'count_corp1': counter1_wordcount,\n",
        "                'count_corp2': counter2_wordcount,\n",
        "                'freq_total': (counter1_wordcount + counter2_wordcount) / (total_words_counter1 +\n",
        "                                                                           total_words_counter2),\n",
        "                'freq_corp1': counter1_wordcount / total_words_counter1,\n",
        "                'freq_corp2': counter2_wordcount / total_words_counter2\n",
        "            }\n",
        "\n",
        "    return dunning_result"
      ],
      "metadata": {
        "id": "ywZlnJ8RAsYT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dunning_log_likelihood_test(count_dict_1, count_dict_2):\n",
        "\n",
        "  # runs the comparison\n",
        "  dunning_comparison = dunning_total(count_dict_1, count_dict_2)\n",
        "\n",
        "  # extracting the dunning values for each word\n",
        "  dunning_values = {}\n",
        "  for term in dunning_comparison:\n",
        "    dunning_values[term] = dunning_comparison[term]['dunning']\n",
        "\n",
        "  return dunning_comparison, dunning_values"
      ],
      "metadata": {
        "id": "UGHOULPSNOSn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_dunning_dict(dunning_values, length = ''):\n",
        "  # the sorted keys of the dunning_values dictionary\n",
        "  sorted_keys_desc = sorted(dunning_values, key = dunning_values.get, reverse = True)\n",
        "  sorted_keys_asc = sorted(dunning_values, key = dunning_values.get, reverse = False)\n",
        "\n",
        "  top_dunning_dict_desc = {}\n",
        "  top_dunning_dict_asc = {}\n",
        "\n",
        "  # if no dictionary length specified or specified length too long, do all\n",
        "  if length == '' or length > len(sorted_keys_desc):\n",
        "    length = len(sorted_keys_desc)\n",
        "\n",
        "  for i in range(length):\n",
        "    cur_word_desc = sorted_keys_desc[i]\n",
        "    cur_word_asc = sorted_keys_asc[i]\n",
        "\n",
        "    top_dunning_dict_desc[cur_word_desc] = dunning_values[cur_word_desc]\n",
        "    top_dunning_dict_asc[cur_word_asc] = dunning_values[cur_word_asc]\n",
        "\n",
        "  return top_dunning_dict_desc, top_dunning_dict_asc"
      ],
      "metadata": {
        "id": "CX-YBmgkDLUR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(vector_1, vector_2):\n",
        "  return 1 - spatial.distance.cosine(vector_1, vector_2)"
      ],
      "metadata": {
        "id": "i82t-kMmfO0S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_vectors(list_of_vectors):\n",
        "\n",
        "  # a list to contain each vector's similarity to every other\n",
        "  vector_sim_list = [[1]*len(list_of_vectors) for i in range(len(list_of_vectors))]\n",
        "\n",
        "  # the indices and similarity value of the most similar vectors\n",
        "  closest_vector_indices = []\n",
        "  closest_sim = float('inf')\n",
        "\n",
        "  # iterates through the vectors\n",
        "  for i in range(len(list_of_vectors)):\n",
        "    for j in range(len(list_of_vectors)):\n",
        "      if i > j:\n",
        "        continue\n",
        "\n",
        "      vector_1 = list_of_vectors[i]\n",
        "      vector_2 = list_of_vectors[j]\n",
        "\n",
        "      cos_sim = compute_cosine_similarity(vector_1, vector_2)\n",
        "\n",
        "      if cos_sim < closest_sim:\n",
        "        closest_sim = cos_sim\n",
        "        closest_vector_indices = [i, j]\n",
        "\n",
        "      vector_sim_list[i][j] = cos_sim\n",
        "      vector_sim_list[j][i] = cos_sim\n",
        "\n",
        "  return vector_sim_list, closest_vector_indices"
      ],
      "metadata": {
        "id": "Ifl8CufCfZd2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_vectors_between_lists(list_1, list_2):\n",
        "  closest_vector_indices = []\n",
        "  closest_sim = float('inf')\n",
        "\n",
        "  for i in range(len(list_1)):\n",
        "    vector_1 = list_1[i]\n",
        "    for j in range(len(list_2)):\n",
        "      vector_2 = list_2[j]\n",
        "      cos_sim = compute_cosine_similarity(vector_1, vector_2)\n",
        "\n",
        "      if cos_sim < closest_sim:\n",
        "        closest_vector_indices = [i, j]\n",
        "        closest_sim = cos_sim\n",
        "\n",
        "  return closest_vector_indices, closest_sim"
      ],
      "metadata": {
        "id": "nh-ESuvRztQa"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "wqjsdlKlVcNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary test"
      ],
      "metadata": {
        "id": "-MekxWEjm1-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### finding the top words of the two corpora based on TF-IDF, and taking the words at their intersection\n",
        "\n",
        "# getting a dictionary of the top words in each corpus\n",
        "explainer_top_words_dict = sort_top_words(explainer_dtm_df.iloc[:, 6:])\n",
        "fiction_top_words_dict = sort_top_words(fiction_dtm_df.iloc[:, 6:])\n",
        "\n",
        "# the top 1000 words from each corpus\n",
        "explainer_top_words = list(explainer_top_words_dict.keys())[:1000]\n",
        "fiction_top_words = list(fiction_top_words_dict.keys())[:1000]\n",
        "\n",
        "# TO DO: RE-SORT?\n",
        "common_words = list(set(explainer_top_words).intersection(set(fiction_top_words)))\n",
        "\n",
        "# dfs with only the common words as column titles\n",
        "explainer_common_words_df = refine_df_columns(explainer_dtm_df, common_words)\n",
        "fiction_common_words_df = refine_df_columns(fiction_dtm_df, common_words)"
      ],
      "metadata": {
        "id": "Zm-pdufzUQ_F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### logistic regression with l2 regularization, 5-fold cross validation on word frequency\n",
        "\n",
        "word_accuracies, word_miscategorized_files, word_closest_vectors, word_sim_values = run_k_cross_validation(explainer_common_words_df, fiction_common_words_df)\n",
        "word_avg_accuracy = round(sum(word_accuracies)/len(word_accuracies), 2)\n",
        "\n",
        "print('The model was ' + str(word_avg_accuracy) + '% accurate (on average)')"
      ],
      "metadata": {
        "id": "vOJMX427xDaL",
        "outputId": "a66623c0-6a72-4947-f89b-10a29a7f5ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model was 98.33% accurate (on average)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### finding closest miscateogrized files\n",
        "\n",
        "word_min_sim = word_sim_values.index(min(word_sim_values))\n",
        "word_min_1 = word_closest_vectors[word_min_sim][0]\n",
        "word_min_2 = word_closest_vectors[word_min_sim][1]\n",
        "\n",
        "print(f'The closest files are: \\n\\t{word_min_1} and \\n\\t{word_min_2} \\nwith a cosine similarity of {min(word_sim_values)}')"
      ],
      "metadata": {
        "id": "rzvkeoVw9wV0",
        "outputId": "68b22305-8ed2-4671-a1c8-6a2d3c239db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The closest files are: \n",
            "\tMASSIVE_SCI/BacillusBacteriaCanKeepFruitsAnd.txt and \n",
            "\tTIN_HOUSE/ForAllOfHumanHistory.txt \n",
            "with a cosine similarity of 0.018993429409939644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech (POS) test"
      ],
      "metadata": {
        "id": "wg1QjH41ViWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### finding the common POS tags and refining the dataframes\n",
        "\n",
        "# finding the POS tags that both corpora have in common (should be most)\n",
        "explainer_pos_tags = explainer_pos_df.columns[10:]\n",
        "fiction_pos_tags = fiction_pos_df.columns[10:]\n",
        "common_tags = list(set(explainer_pos_tags).intersection(set(fiction_pos_tags)))\n",
        "\n",
        "# dfs with only common words as column titles\n",
        "explainer_common_tags_df = refine_df_columns(explainer_pos_df, common_tags)\n",
        "fiction_common_tags_df = refine_df_columns(fiction_pos_df, common_tags)"
      ],
      "metadata": {
        "id": "zCFHOyx1d222"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### logistic regression with l2 regularization, 5-fold cross validation on tag frequency\n",
        "\n",
        "tag_accuracies, tag_miscategorized_files, tag_closest_vectors, tag_sim_values = run_k_cross_validation(explainer_common_tags_df, fiction_common_tags_df)\n",
        "tag_avg_accuracy = round(sum(tag_accuracies)/len(tag_accuracies), 2)\n",
        "\n",
        "print('The model was ' + str(tag_avg_accuracy) + '% accurate (on average)')"
      ],
      "metadata": {
        "id": "MOmPCTqeV7jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c01aa77-d943-412a-93fc-d0ac654e8b97"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model was 98.0% accurate (on average)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### finding closest miscateogrized files\n",
        "\n",
        "tag_min_sim = tag_sim_values.index(min(tag_sim_values))\n",
        "tag_min_1 = tag_closest_vectors[tag_min_sim][0]\n",
        "tag_min_2 = tag_closest_vectors[tag_min_sim][1]\n",
        "\n",
        "print(f'The closest files are: \\n\\t{tag_min_1} and \\n\\t{tag_min_2} \\nwith a cosine similarity of {min(tag_sim_values)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QpgsMOwQt5e",
        "outputId": "3a7e4952-40b6-4591-9ae5-41e18b8ed393"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The closest files are: \n",
            "\tMASSIVE_SCI/MeetTheWartyCombJellyThe.txt and \n",
            "\tTIN_HOUSE/ExclusivePreviewLevarBurtonReadsSea.txt \n",
            "with a cosine similarity of 0.7240372375328364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More tests"
      ],
      "metadata": {
        "id": "MUT6_lGkVUUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood on word counts\n",
        "\n"
      ],
      "metadata": {
        "id": "TfBYG0M0Adty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### comparing the statistical frequency of words in each corpus\n",
        "\n",
        "explainer_pure_counts_dict = sort_top_words(explainer_dtm_df.iloc[:, 6:], normalize = False)\n",
        "fiction_pure_counts_dict = sort_top_words(fiction_dtm_df.iloc[:, 6:], normalize = False)\n",
        "\n",
        "dunning_comparison_words, dunning_values_words, = run_dunning_log_likelihood_test(explainer_pure_counts_dict, fiction_pure_counts_dict)\n",
        "top_dunning_dict_words_desc, top_dunning_dict_words_asc = create_top_dunning_dict(dunning_values_words, length = 30)"
      ],
      "metadata": {
        "id": "VMQxJzGBOC6p"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Top 30 words stastistically associated with science explainers:\\n')\n",
        "top_dunning_dict_words_desc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS-UQUFyQIJ0",
        "outputId": "3a8f2c3b-51a2-4810-8704-9e253c228408"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 words stastistically associated with science explainers:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'study': 5580.025421059754,\n",
              " 'climate': 3563.185631680107,\n",
              " 'research': 3465.1573753819603,\n",
              " 'researcher': 3253.7455586974447,\n",
              " 'scientist': 2812.620654932036,\n",
              " 'health': 2425.309408064606,\n",
              " 'university': 2374.466378751723,\n",
              " 'specie': 2349.5518924630223,\n",
              " 'data': 1851.1718163552014,\n",
              " 'canada': 1766.2639218706327,\n",
              " 'human': 1754.7917983461257,\n",
              " 'change': 1546.1083488317727,\n",
              " 'risk': 1346.5570681135284,\n",
              " 'science': 1319.4956722336642,\n",
              " 'million': 1316.9167454395706,\n",
              " 'impact': 1247.4170623469613,\n",
              " 'disease': 1238.5184746790167,\n",
              " 'national': 1230.926292670977,\n",
              " 'carbon': 1227.502161486048,\n",
              " 'ha': 1158.9319868151683,\n",
              " 'based': 1120.0684829765855,\n",
              " 'team': 1092.0024920305038,\n",
              " 'animal': 1062.6106724715046,\n",
              " 'including': 1036.7519560211058,\n",
              " 'global': 1035.149118539202,\n",
              " 'according': 994.5388151173044,\n",
              " 'published': 970.4381896461853,\n",
              " 'environmental': 958.4075115879962,\n",
              " 'population': 955.8411442532463,\n",
              " 'temperature': 936.7127087688812}"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Top 30 words stastistically associated with short stories:\\n')\n",
        "top_dunning_dict_words_asc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137asphLQFS2",
        "outputId": "8705d62d-49be-4604-c50c-c2caffe8dffb"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 words stastistically associated with short stories:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wa': -9840.33036448821,\n",
              " 'mother': -2261.6082928863093,\n",
              " 'father': -2041.5025304082028,\n",
              " 'man': -1828.0174570702395,\n",
              " 'didn': -1749.6014230404667,\n",
              " 'hand': -1415.5778426475267,\n",
              " 'door': -1204.8985773367133,\n",
              " 'did': -1167.768244273509,\n",
              " 'room': -1154.6646539529186,\n",
              " 'boy': -1144.581182604431,\n",
              " 'asked': -1140.8198712615927,\n",
              " 'house': -1107.5745019910692,\n",
              " 'knew': -1019.2991529828855,\n",
              " 'know': -984.8551473410625,\n",
              " 'head': -982.6454721185675,\n",
              " 'felt': -961.5267116817364,\n",
              " 'thought': -954.2626050215027,\n",
              " 'like': -952.581124463904,\n",
              " 'wanted': -919.0254710404631,\n",
              " 'girl': -915.0105223925628,\n",
              " 'went': -896.1636707477646,\n",
              " 'tell': -875.6526456229028,\n",
              " 'face': -870.5236839996028,\n",
              " 'woman': -859.1700028223601,\n",
              " 'night': -827.2030828753378,\n",
              " 'looked': -826.9663023650298,\n",
              " 'got': -788.1473430075921,\n",
              " 'wife': -767.460834001947,\n",
              " 'll': -750.0220523889813,\n",
              " 'let': -728.4800734902414}"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### preparing to do predictive modelling again\n",
        "\n",
        "# the top 60 defining words from the respective corpora\n",
        "top_60_words = list(top_dunning_dict_words_desc.keys()) + list(top_dunning_dict_words_asc.keys())\n",
        "\n",
        "# dfs with only common words as column titles\n",
        "explainer_top_words_df = refine_df_columns(explainer_dtm_df, top_60_words)\n",
        "fiction_top_words_df = refine_df_columns(fiction_dtm_df, top_60_words)"
      ],
      "metadata": {
        "id": "FEPF_8QDRHDP"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### logistic regression with l2 regularization, 5-fold cross validation on tag frequency\n",
        "\n",
        "top_words_accuracies, top_words_miscategorized_files, top_words_closest_vectors, top_words_sim_values = run_k_cross_validation(explainer_top_words_df, fiction_top_words_df)\n",
        "top_words_avg_accuracy = round(sum(top_words_accuracies)/len(top_words_accuracies), 2)\n",
        "\n",
        "print('The model was ' + str(top_words_avg_accuracy) + '% accurate (on average)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krCVcOG7Rqyn",
        "outputId": "1c8a53f2-5a22-4201-998f-c941b49a8ecf"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model was 98.66% accurate (on average)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### finding closest miscateogrized files\n",
        "\n",
        "top_words_min_sim = top_words_sim_values.index(min(top_words_sim_values))\n",
        "top_words_min_1 = top_words_closest_vectors[top_words_min_sim][0]\n",
        "top_words_min_2 = top_words_closest_vectors[top_words_min_sim][1]\n",
        "\n",
        "print(f'The closest files are: \\n\\t{top_words_min_1} and \\n\\t{top_words_min_2} \\nwith a cosine similarity of {min(top_words_sim_values)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt_jKO3SU6ZR",
        "outputId": "afad174c-c48d-44f9-9324-324b32d10773"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The closest files are: \n",
            "\tMASSIVE_SCI/DarkMatterMakesUpAQuarter.txt and \n",
            "\tTIN_HOUSE/NaturalOrder.txt \n",
            "with a cosine similarity of 0.3499671046381734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood + topic modelling"
      ],
      "metadata": {
        "id": "D5B3_owcX9AM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood on verbs"
      ],
      "metadata": {
        "id": "IsZiKtR5Fi26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunning log likelihood on POS tags"
      ],
      "metadata": {
        "id": "Y9yXYfxHFkqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### comparing the statistical frequency of words in each corpus\n",
        "\n",
        "explainer_pure_pos_dict = sort_top_words(explainer_pos_df.iloc[:, 9:], normalize = False)\n",
        "fiction_pure_pos_dict = sort_top_words(fiction_pos_df.iloc[:, 9:], normalize = False)\n",
        "\n",
        "dunning_comparison_pos, dunning_values_pos, = run_dunning_log_likelihood_test(explainer_pure_pos_dict, fiction_pure_pos_dict)\n",
        "top_dunning_dict_pos_desc, top_dunning_dict_pos_asc = create_top_dunning_dict(dunning_values_pos, length = 15)"
      ],
      "metadata": {
        "id": "DrIvvSifF1ZH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### POS tags associated with science explainer corpus\n",
        "print('Top 15 POS tags stastistically associated with science explainers:\\n')\n",
        "top_dunning_dict_pos_desc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_3dgC-5MsBv",
        "outputId": "dcc65866-271d-43d0-e526-391cf36d36e4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 POS tags stastistically associated with science explainers:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NNS': 26057.40560076355,\n",
              " 'NNP': 11880.227142034673,\n",
              " 'CD': 10358.688977236981,\n",
              " 'POS': 5866.264038410277,\n",
              " 'JJ': 3695.9474049878736,\n",
              " 'VBZ': 2706.8421883201554,\n",
              " 'WDT': 2395.65120182971,\n",
              " 'JJR': 2373.730070614861,\n",
              " 'NNPS': 1938.1229419804401,\n",
              " 'IN': 1882.0549391403838,\n",
              " 'RBR': 1018.112661011126,\n",
              " 'JJS': 742.0706699665345,\n",
              " 'RBS': 689.8235226748163,\n",
              " 'VBN': 538.1044151086189,\n",
              " 'MD': 258.905485093258}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### POS tags associated with short fiction corpus\n",
        "print('Top 15 POS tags stastistically associated with short stories:\\n')\n",
        "top_dunning_dict_pos_asc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYbcxkMM6f7",
        "outputId": "e4d341e9-c49c-4466-ec76-ae9513b15c79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 POS tags stastistically associated with short stories:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PRP': -58501.13846990699,\n",
              " 'VBD': -33769.209531262546,\n",
              " 'PRP$': -15658.14752117962,\n",
              " 'RP': -1875.6255851084898,\n",
              " 'RB': -1667.2321902912172,\n",
              " 'WP': -751.8584655212762,\n",
              " 'UH': -646.7086309936711,\n",
              " 'WRB': -487.07678088331113,\n",
              " 'PDT': -308.55895399069243,\n",
              " 'CC': -216.6937950232268,\n",
              " 'VB': -114.69331468380369,\n",
              " 'EX': -24.95786316926302,\n",
              " 'WP$': -6.8933092378428,\n",
              " 'VBP': -0.05825205682171486,\n",
              " 'NN': 0.49129599942045843}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stats testing"
      ],
      "metadata": {
        "id": "cLXj3SowUCPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "e_col = explainer_dtm_df.columns\n",
        "f_col = fiction_dtm_df.columns\n",
        "\n",
        "same_cols = list(set(e_col).intersection(set(f_col)))\n",
        "\n",
        "explainer_stats = refine_df_columns(explainer_dtm_df, same_cols)\n",
        "fiction_stats = refine_df_columns(fiction_dtm_df, same_cols)\n",
        "\n",
        "e_val = explainer_stats.values.tolist()\n",
        "f_val = fiction_stats.values.tolist()\n",
        "\n",
        "U1, p = mannwhitneyu(e_val, f_val, method=\"exact\")"
      ],
      "metadata": {
        "id": "AdQVSW6JSapK"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}